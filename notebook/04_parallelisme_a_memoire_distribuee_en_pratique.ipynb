{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallélisme à mémoire distribuée "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook et parallélisme à mémoire distribuée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration de votre environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de pouvoir faire du parallélisme à mémoire partagé en Python et plus particulièrement dans des notebooks jupyter il va falloir faire quelques installations. \n",
    "\n",
    "La première étapes va être de vous créer un environnement conda dédié à ce cours, ce qui vous permettra de ne pas polluer votre environnement par défaut. \n",
    "\n",
    "Pour cela ouvrez un terminal et tapez\n",
    "\n",
    "```bash \n",
    "conda create -n parallel python=3.7\n",
    "conda activate parallel\n",
    "``` \n",
    "\n",
    "Nous allons ensuite installer l'ensemble de modules python que nous allons utiliser dans cette partie \n",
    "```bash \n",
    "conda install numpy matplotlib mpi4py jupyter \n",
    "```\n",
    "\n",
    "Il vous faut ensuite activer l'extension jupyter permettant de faire du parallèle dans les notebooks. \n",
    "\n",
    "```bash \n",
    "jupyter serverextension enable --py ipyparallel\n",
    "```\n",
    "\n",
    "C'est cette extension qui va vous permettre de créer un cluster local vous permettant de faire du parallélisme dans vos notebook. Il vous faut pour cela configurer votre \"cluster\" en utilisant la commande \n",
    "\n",
    "```bash \n",
    "ipython profile create --parallel --profile=mpi\n",
    "```\n",
    "\n",
    "Il faut ensuite que vous modifiez le fichier `$HOME/.ipython/profile_mpi/ipcluster_config.py`pour lui ajouter la ligne suivante \n",
    "\n",
    "```python\n",
    "c.IPClusterEngines.engine_launcher_class = 'MPIEngineSetLauncher'\n",
    "```\n",
    "\n",
    "A partir de maintenant votre configuration est terminée. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test de votre environnement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons à présent vérifier que votre environnement parallèle fonctionne correctement. Pour cela il faut dans un premier temps lancer le server. Pour cela ouvrez un nouveau terminal et tapez\n",
    "\n",
    "```bash \n",
    "conda activate parallel ## Pour activer le bon environement conda\n",
    "ipcluster start --profile=mpi\n",
    "```\n",
    "\n",
    "Si tout fonctionne bien vous devez avoir en dernière ligne du message la ligne suivante :\n",
    "\n",
    "```\n",
    "[IPClusterStart] Engines appear to have started successfully\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons ensuite initializer le client de notre cluster de calcul pour le notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import ipyparallel as parallel\n",
    "clients = parallel.Client(profile=\"mpi\")\n",
    "clients.block = False  # use synchronous computations\n",
    "print(clients.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que le cluster se composer de 4 coeurs ce qui correspond au nombre de coeurs disponible sur mon ordinateur portable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour vérifier que tout est opérationel il nous suffit alors de regarder si notre communicateur MPI a bien la bonne taille, i.e établie bien la communication entre nos 4 coeurs. Pour cela on peut appeler le code suivant et si tout fonctionne on doit sur chaque coeurs avoir un communicateur de taille 4. Si c'est le cas c'est que tout fonctionne correctement et on va pouvoir passer aux choses sérieuses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] 4\n",
      "[stdout:1] 4\n",
      "[stdout:2] 4\n",
      "[stdout:3] 4\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "from mpi4py import MPI\n",
    "\n",
    "print(MPI.COMM_WORLD.Get_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention :** il faut bien noter le fait que la cellule commence par la *magic command* `%%px` qui signifie à jupyter que la cellule en question doit être éxécutée en parallèle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message Passing Interface en Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mpi4py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous l'avons vu dans le notebook 2 la seconde manière d'exploiter les architectures de calcul moderne est de faire du parallélisme à mémoire distribuée. Nous avons également vu qu'il existe ou a existé un certain nombre de manière de faire ce parallélisme mais que depuis maintenant de nombreuses année une approche est privilégiée c'est d'utiliser le Message Passing Interface (MPI). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'implémentation de MPI en python est fait dans le module mpi4py qui est en réalité une surcouche Python des implémentation MPI en C classiques. Elle permet donc d'avoir directement dans Python toute les possibilités de MPI et en plus dans un format plus sympatique à utiliser que ce que l'on trouve dans les langages de plus bas niveau type C, C++, FORTRAN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus précisément pour utiliser MPI dans Python il faut utiliser le sous-module MPI du module mpi4py. Son import se fait de la manière classique suivante : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from mpi4py import MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notion de communicateur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le paradigme MPI l'élément centrale du développement est ce que l'on appel le **communicateur**. Il porte bien son nom puisque son seul rôle est de définir qui communique avec qui, plus précisément un communicateur représente un ensemble de processus MPI pouvant communiquer entre eux. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe un comminucateur par défaut dans MPI qui s'appelle `COMM_WORLD` et qui la encore porte bien son nom puisqu'il s'agit du communicateur comprenant l'ensemble des processus MPI lancés. C'est ce communicateur qui va permettre à tous les processus de communiquer entre eux, soit en mode point à point, soit en mode collectif.\n",
    "\n",
    "Dans `mpi4py` pour récupérer le communicateur il suffit simplement de récupérer le variable `COMM_WORLD` du module `MPI`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] <mpi4py.MPI.Intracomm object at 0x7fb536451b90>\n",
      "[stdout:1] <mpi4py.MPI.Intracomm object at 0x7efd7797ab90>\n",
      "[stdout:2] <mpi4py.MPI.Intracomm object at 0x7f1d5af88b90>\n",
      "[stdout:3] <mpi4py.MPI.Intracomm object at 0x7f806cacdb90>\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "comm = MPI.COMM_WORLD\n",
    "print(comm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de ce communicateur nous pouvons déjà déterminer :\n",
    "* La taille du communicateur, i.e. le nombre de processus qu'il met en relation\n",
    "* Le rank de chaque processus dans ce communicateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] Je suis le processus 0 sur 4\n",
      "[stdout:1] Je suis le processus 1 sur 4\n",
      "[stdout:2] Je suis le processus 2 sur 4\n",
      "[stdout:3] Je suis le processus 3 sur 4\n"
     ]
    }
   ],
   "source": [
    "%%px \n",
    "print( f\"Je suis le processus {comm.Get_rank()} sur {comm.Get_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depuis le début je vous parle d'un communicateur et non pas du communicateur. C'est pour la simple et bonne raison que le communicateur COMM_WORLD n'est pas unique. Il s'agit juste du communicateur par défaut mais on peut en créer d'autre. On parle généralement de sous-communicateur. Quel intérêt vous demandez vous ? Cela dépends du problème que l'on traite. Mais il peut arriver que l'on ait besoin de faire des opérations de communications sur un sous-ensemble de processus MPI. Dans ce cas il est généralement plus malin de créer un sous-communicateur regroupant les processus concerné. \n",
    "\n",
    "Par exemple imaginons que l'on veuille que nos processus de rang pair communique qu'entre eux et de même pour nos processus de rang impair. La solution serait de créer un sous-comminucateur pour les processus de rang pair et un autre pour les processus de rang impair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] Je suis le processus pair 0 sur 2\n",
      "[stdout:1] Je suis le processus impair 0 sur 2\n",
      "[stdout:2] Je suis le processus pair 1 sur 2\n",
      "[stdout:3] Je suis le processus impair 1 sur 2\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "rank = comm.Get_rank()\n",
    "grp = comm.Get_group()\n",
    "grp_impair = grp.Excl([0,2])\n",
    "comm_impair = comm.Create_group(grp_impair)\n",
    "grp_pair = grp.Excl([1,3])\n",
    "comm_pair = comm.Create_group(grp_pair)\n",
    "if rank%2 != 0:\n",
    "    print( f\"Je suis le processus impair {comm_impair.Get_rank()} sur {comm_impair.Get_size()}\")\n",
    "else:\n",
    "    print( f\"Je suis le processus pair {comm_pair.Get_rank()} sur {comm_pair.Get_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De cette manière on peut facilement échanger des données entre processus pairs, processus impair ou l'ensemble des processus. Cela permet dans certains cas de figure d'optimiser le développement et de minimiser les communications. Car comme cela a été montré dans le notebook 2 les communications entre processus MPI ont un coût pouvant être non négligeable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien évidemment ce n'est ici qu'une présentation succinte de ce qu'il est possible de faire avec les communicateur. Pour avoir de plus amples informations je vous conseil de lire le cours de l'IDRIS [http://www.idris.fr/media/formations/mpi/idrismpi.pdf](http://www.idris.fr/media/formations/mpi/idrismpi.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C'est jolie le communicateur mais pour le moment on a pas encore communiqué ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous vous dites certainement que c'est bien beau les communicateurs mais que pour le moment vous n'êtes toujours pas capable d'échanger la moindre information entre vos processus MPI et donc que vous ne voyez pas l'intérêt. Je vous répondrai à cela vous avez raison ! Mais nous allons y remédier tout de suite. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord il est important de noter qu'il existe en MPI deux types de communications : \n",
    "\n",
    "* Les communications dites **Point-to-point** : la communication n'a lieu qu'entre deux processus, l'un dit le processus émetteur et le second dit le processus récepteur.\n",
    "* Les communications dites **Collectives** : la communication a lieu entre ***tous*** les processus du communicateur utilisé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Communications *Point-to-Point*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons donc commencer par le plus simple à savoir les communications *Point-to-Point*. Comme nous l'avons déjà dit il s'agit de communication ne faisant intervenir que deux processus un émetteur et un récepteur. Dans ce modèle de communication il n'y a donc que deux fonction à connaitre : \n",
    "\n",
    "* `send` pour envoyer \n",
    "* `recv` pour recevoir \n",
    "\n",
    "A partir de ces deux commandes vous savez tout faire ! Nous allons voir qu'en réalité dans l'interface Python, j'insiste sur le fait que c'est une spécificité de mpi4py, il y a en fait 4 fonctions à connaitre `send`, `Send`, `recv` et `Recv`. \n",
    "Euuuh c'est quoi la différence ? La majuscule !! Et comme on dit le diable est dans le détails. \n",
    "\n",
    "Pour expliquer simplement ce qui se cache dans cette majuscule nous pouvons dire que `send` et `recv` sont des versions très Pythonesque de l'implémentation Python de MPI. Ces fonctions vont permettre d'envoyer/recevoir n'importe quel type Python. C'est super du coup on peut envoyer tout et n'importe quoi ? Oui c'est vrai mais cela à un prix, le prix c'est la performance. En effet pour réussir à envoyer n'importe quel type via MPI la fonction `send` procède tout d'abord à une sérialization de l'objet à envoyer en utilisant le module `pickle`. C'est universel ça fonctionne mais en revanche ca génère potentiellement un gros volume de données à envoyer donc des communications plus coûteuses.\n",
    "\n",
    "Par opposition les deux fonctions `Send` et `Receive` sont plus proche de l'implémentation C de MPI dans le sens où il faut leurs fournir le type de ce que l'on veut envoyer, par exemple des entiers ou des flottants. Ainsi il n'y a pas de sérialization mais un appel direct à la librairie MPI C. De ce fait les données à envoyer sont plus légères et donc les communications moins coûteuses. En revanche ces fonctions nécessitent de n'utiliser que des tableaux `numpy`. \n",
    "\n",
    "Pour illustrer cela regardons l'occupation mémoire d'une liste de 100 flottants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of a = 920\n",
      "Size of a_np = 896\n",
      "Size of a serialized = 941\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "a = [ 0.01 * x for x in range(100)]   ### Une liste Python classique\n",
    "a_np = np.asarray(a)                  ### La même chose en tableau numpy \n",
    "a_serialized = pickle.dumps(a)        ### On sérialize la liste python \n",
    "\n",
    "print(f\"Size of a = {sys.getsizeof(a)}\")\n",
    "print(f\"Size of a_np = {sys.getsizeof(a_np)}\")\n",
    "print(f\"Size of a serialized = {sys.getsizeof(a_serialized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate donc que la liste sérializée est l'objet qui prend le plus de place en mémoire par rapport au tableau numpy qui pourra être envoyé tel quel sans sérialization. Il est donc préférable de manière générale quand on fait du mpi4py de travailler avec des tableaux numpy plutot que les types de base Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] Proc 0 send [0.38288584 0.79795476 0.76369463 0.66726267 0.33108507] to 1\n",
      "[stdout:1] Proc 1 receive [0.38288584 0.79795476 0.76369463 0.66726267 0.33108507] from 0\n",
      "[stdout:2] Nothing to do\n",
      "[stdout:3] Nothing to do\n"
     ]
    }
   ],
   "source": [
    "%%px \n",
    "import numpy as np\n",
    "rank = comm.Get_rank()\n",
    "n = 5\n",
    "if rank==0:\n",
    "    data_to_send = np.random.rand(n)\n",
    "    print(f\"Proc {rank} send {data_to_send} to 1\")\n",
    "    comm.Send(data_to_send, 1)\n",
    "elif rank==1:\n",
    "    data_to_receive = np.empty(n)\n",
    "    comm.Recv(data_to_receive, 0)\n",
    "    print(f\"Proc {rank} receive {data_to_receive} from 0\")  \n",
    "else: # Proc 2 and 3 \n",
    "    print(\"Nothing to do\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut cependant faire attention à une chose !!! Les opérations `Send` et `Recv` (tout comme `send` et `recv`) sont des opérations ***bloquantes***. Il faut donc faire attention car si vous faites un `Send` et qu'il n'y a personne à la réception avec `Recv` le processus faisant le `Send` restera bloqué sans jamais passé à la suite. \n",
    "\n",
    "Par exemple si vous voules faire un échange de donner entre deux processus faites bien attention au sens dans lequel vous faites les `Send`, `Recv`. La bonne solution étant la suivante : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "Proc 0 send [0.02523557 0.16762661 0.88968241 0.53751162 0.23873169] to 1\n",
      "Proc 0 wait for receive\n",
      "Proc 0 receive [0.41731322 0.6498673  0.79985255 0.99301029 0.79135858] from 1\n",
      "[stdout:1] \n",
      "Proc 1 wait for receive\n",
      "Proc 1 receive [0.02523557 0.16762661 0.88968241 0.53751162 0.23873169] from 0\n",
      "Proc 1 send [0.41731322 0.6498673  0.79985255 0.99301029 0.79135858] to 0\n",
      "[stdout:2] Nothing to do\n",
      "[stdout:3] Nothing to do\n"
     ]
    }
   ],
   "source": [
    "%%px \n",
    "rank = comm.Get_rank()\n",
    "n = 5\n",
    "data_to_send = np.random.rand(n)\n",
    "data_to_receive = np.empty(n)\n",
    "if rank==0:\n",
    "    print(f\"Proc {rank} send {data_to_send} to 1\")\n",
    "    comm.Send(data_to_send, 1)\n",
    "    print(f\"Proc {rank} wait for receive\")\n",
    "    comm.Recv(data_to_receive, 1)\n",
    "    print(f\"Proc {rank} receive {data_to_receive} from 1\")\n",
    "elif rank==1:\n",
    "    print(f\"Proc {rank} wait for receive\")\n",
    "    comm.Recv(data_to_receive, 0)\n",
    "    print(f\"Proc {rank} receive {data_to_receive} from 0\")  \n",
    "    print(f\"Proc {rank} send {data_to_send} to 0\")\n",
    "    comm.Send(data_to_send, 0)\n",
    "    \n",
    "else: # Proc 2 and 3 \n",
    "    print(\"Nothing to do\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je le reconnais c'est un peu lourd comme syntaxe pour s'échanger deux tableaux de 5 flottants. Mais pas d'inquiétude il y a bien évidemment une astuce ! Si vous voulez échanger deux données entre deux processus vous pouvez utiliser la fonction  `Sendrecv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "Proc 0 send [0.54985617 0.83978181 0.43817786 0.67197669 0.11831248] to 1\n",
      "Proc 0 receive [0.92152755 0.46032641 0.48710679 0.76919806 0.67271042]\n",
      "[stdout:1] \n",
      "Proc 1 send [0.92152755 0.46032641 0.48710679 0.76919806 0.67271042] to 0\n",
      "Proc 1 receive [0.54985617 0.83978181 0.43817786 0.67197669 0.11831248]\n"
     ]
    }
   ],
   "source": [
    "%%px \n",
    "rank = comm.Get_rank()\n",
    "n = 5\n",
    "data_to_send = np.random.rand(n)\n",
    "data_to_receive = np.empty(n)\n",
    "\n",
    "dest = -1\n",
    "if rank==0:\n",
    "    dest = 1\n",
    "elif rank==1:\n",
    "    dest = 0\n",
    "\n",
    "if dest != -1:\n",
    "    print(f\"Proc {rank} send {data_to_send} to {dest}\")\n",
    "    comm.Sendrecv(data_to_send, dest, recvbuf=data_to_receive)\n",
    "    print(f\"Proc {rank} receive {data_to_receive}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et afin de simplifier encore le processus d'échange il existe la fonction `Sendrecv_replace` qui fait l'échange en place sans avoir besoin de buffer pour la réception. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got unknown result: 9f2b2152-ef4f9f294f237c083849efb2_44\n",
      "[stdout:0] \n",
      "Proc 0 send [0.68194334 0.06371774 0.54144765 0.77229978 0.06658316] to 1\n",
      "Proc 0 receive [0.5487634  0.51796844 0.49085789 0.75965757 0.71794603]\n",
      "[stdout:1] \n",
      "Proc 1 send [0.5487634  0.51796844 0.49085789 0.75965757 0.71794603] to 0\n",
      "Proc 1 receive [0.68194334 0.06371774 0.54144765 0.77229978 0.06658316]\n"
     ]
    }
   ],
   "source": [
    "%%px \n",
    "rank = comm.Get_rank()\n",
    "n = 5\n",
    "data = np.random.rand(n)\n",
    "\n",
    "dest = -1\n",
    "if rank==0:\n",
    "    dest = 1\n",
    "elif rank==1:\n",
    "    dest = 0\n",
    "\n",
    "if dest != -1:\n",
    "    print(f\"Proc {rank} send {data} to {dest}\")\n",
    "    comm.Sendrecv_replace(data, dest)\n",
    "    print(f\"Proc {rank} receive {data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec ces éléments vous avez toutes les billes nécessaires pour faire de la programmation parallèle de base avec MPI. C'est cependant loin d'être tout ce qu'offre la norme MPI et nous allons voir cela tout de suite avec les communications collectives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque :** Précédemment j'ai insisté sur le fait que les communications dans MPI sont bloquantes et que par conséquent il faut bien faire attention lorsque l'on programme à ce qu'un `Send` sur un processus soit toujours associé à un `Recv` sur un autre processus. Il existe en réalité un moyen de faire des communications non-bloquantes dans MPI, c'est ce qui permet notamment de faire de la programmation asynchrone distribuée. C'est en revance relativement complexe et nécessite un peu d'expérience. Dans le cadre de ce cours nous ne nous intéresserons donc pas aux communications non bloquantes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Communications collective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous venons de voir comment faire des communications entre deux processus MPI, avec cela nous pouvons en théorie faire tout et n'importe quoi. C'est vrai. Néanmoins il existe dans le norme MPI des fonctions chargées de faire des communications globales, par exemple un processus envoie une donnée à tous les autres. Vous pourriez me dire on peut le faire avec plusieurs communications *Point-to-point* ! C'est vrai cela donnerai quelque chose du genre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] Proc 0 send [0.11208368 0.81650901 0.23764493 0.93262656 0.98464805] to 1\n",
      "[stdout:1] Proc 1 receive [0.11208368 0.81650901 0.23764493 0.93262656 0.98464805] from 0\n",
      "[stdout:2] Proc 2 receive [0.11208368 0.81650901 0.23764493 0.93262656 0.98464805] from 0\n",
      "[stdout:3] Proc 3 receive [0.11208368 0.81650901 0.23764493 0.93262656 0.98464805] from 0\n"
     ]
    }
   ],
   "source": [
    "%%px \n",
    "rank = comm.Get_rank()\n",
    "n = 5\n",
    "if rank==0:\n",
    "    data_to_send = np.random.rand(n)\n",
    "    print(f\"Proc {rank} send {data_to_send} to 1\")\n",
    "    for i in range(comm.Get_size()):\n",
    "        if i==rank:\n",
    "            continue ### Le 0 ne peut pas s'envoyer a lui meme\n",
    "        comm.Send(data_to_send, i)\n",
    "else:\n",
    "    data_to_receive = np.empty(n)\n",
    "    comm.Recv(data_to_receive, 0)\n",
    "    print(f\"Proc {rank} receive {data_to_receive} from 0\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code précédent fonctionne et fait le travail attendu. En revanche ce n'est pas ce qu'il y a de plus simple et élégant. De plus on peut facilement faire une erreur, par exemple oublier le `continue` dans la boucle ce qui provoquerai alors un bloquage des processus MPI. Afin de faciliter ce genre d'opération il existe donc dans MPI des fonctions toutes faites permettant de réduire fortement les erreurs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par exemple pour faire l'opération précédente il suffit d'utiliser la fonction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "Before Bcast : Proc 0 - data = [0.94269071 0.77415606 0.72829882 0.6851538  0.5552837 ]\n",
      "After Bcast : Proc 0 - data = [0.94269071 0.77415606 0.72829882 0.6851538  0.5552837 ]\n",
      "[stdout:1] \n",
      "Before Bcast : Proc 1 - data = [0. 0. 0. 0. 0.]\n",
      "After Bcast : Proc 1 - data = [0.94269071 0.77415606 0.72829882 0.6851538  0.5552837 ]\n",
      "[stdout:2] \n",
      "Before Bcast : Proc 2 - data = [0. 0. 0. 0. 0.]\n",
      "After Bcast : Proc 2 - data = [0.94269071 0.77415606 0.72829882 0.6851538  0.5552837 ]\n",
      "[stdout:3] \n",
      "Before Bcast : Proc 3 - data = [0. 0. 0. 0. 0.]\n",
      "After Bcast : Proc 3 - data = [0.94269071 0.77415606 0.72829882 0.6851538  0.5552837 ]\n"
     ]
    }
   ],
   "source": [
    "%%px \n",
    "import numpy as np\n",
    "rank=comm.Get_rank()\n",
    "n=5\n",
    "source_rank=0\n",
    "if rank == source_rank:\n",
    "    data = np.random.rand(5)\n",
    "else:\n",
    "    data = np.zeros(n)\n",
    "\n",
    "print(f\"Before Bcast : Proc {rank} - data = {data}\")\n",
    "comm.Bcast(data, source_rank)\n",
    "print(f\"After Bcast : Proc {rank} - data = {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est ce que l'on appelle une opération de `broadcast`, elle permet donc de propager une information à tous les processus. L'opération de broadcast peut se représenter schématiquement de la manière suivante \n",
    "\n",
    "![bcast](../media/bcast.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le même esprit il y a les opérations de `scatter` permettant de distribuer les éléments d'un tableau sur l'ensemble des processus en accord avec leurs rang. Et il y a également l'opération de `gather` qui permet de collecter sur chaque processus un éléments d'un tableau qui va être centralisé sur un seul processus. Schématique ces deux opérations peuvent se représenter de la manière suivante \n",
    "\n",
    "![scatter_gather](../media/scatter_gather.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'utilisation de scatter et gather se fait de la manière suivante :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "Proc 0 scatter data [0.12131509 0.68673063 0.33814305 0.04418417 0.98130651 0.55005346\n",
      " 0.23743389 0.58214494 0.21386307 0.93165005 0.82919549 0.20992946\n",
      " 0.15233001 0.53034674 0.27851615 0.59268366]\n",
      "After Scatter : Proc 0 - recv = [0.12131509 0.68673063 0.33814305 0.04418417]\n",
      "[stdout:1] After Scatter : Proc 1 - recv = [0.98130651 0.55005346 0.23743389 0.58214494]\n",
      "[stdout:2] After Scatter : Proc 2 - recv = [0.21386307 0.93165005 0.82919549 0.20992946]\n",
      "[stdout:3] After Scatter : Proc 3 - recv = [0.15233001 0.53034674 0.27851615 0.59268366]\n"
     ]
    }
   ],
   "source": [
    "%%px \n",
    "rank=comm.Get_rank()\n",
    "size=comm.Get_size()\n",
    "n=4\n",
    "source_rank=0\n",
    "\n",
    "if rank == source_rank:\n",
    "    data = np.random.rand(size*n)\n",
    "    data.reshape((size, n))\n",
    "    print(f\"Proc {rank} scatter data {data}\")\n",
    "receive_buf = np.zeros(n)\n",
    "\n",
    "comm.Scatter(data, receive_buf, root=source_rank)\n",
    "print(f\"After Scatter : Proc {rank} - recv = {receive_buf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "Proc 0 send [0. 0. 0. 0. 0.]\n",
      "Proc 0 has data : [[0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [2. 2. 2. 2. 2.]\n",
      " [3. 3. 3. 3. 3.]]\n",
      "[stdout:1] Proc 1 send [1. 1. 1. 1. 1.]\n",
      "[stdout:2] Proc 2 send [2. 2. 2. 2. 2.]\n",
      "[stdout:3] Proc 3 send [3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "%%px \n",
    "\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "n=5\n",
    "root_rank = 0\n",
    "sendbuf = np.zeros(n) + rank\n",
    "recvbuf = None\n",
    "if rank == root_rank:\n",
    "    recvbuf = np.empty([size, n])\n",
    "    \n",
    "print(f\"Proc {rank} send {sendbuf}\")\n",
    "comm.Gather(sendbuf, recvbuf, root=root_rank)\n",
    "\n",
    "if rank==root_rank:\n",
    "    print(f\"Proc {rank} has data : {recvbuf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En plus des opérations `scatter` et `gather` il existe également les opérations plus globales `allgather` et `alltoall`. Elles se représentent schématiquement de la manière suivante :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![allgather](../media/allgather.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alltoall](../media/alltoall.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pour finir sur les opérations collectives nous allons voir une fonction extrèmement utile et permettant grandement de se simplifier la vie il s'agit de la fonction `reduce`. Le principe est d'appliquer une opération algébrique à un ensemble d'élement pour obtenir une seule valeur. \n",
    "\n",
    "Par exemple en séquentiel le calcul de la somme des éléments d'une liste peut se faire par l'opération de réduction suivante en Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4,5]\n",
    "sum(a[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée des opérations MPI `reduce` et `allreduce` est la même mais en parallèle distribué. La distinction entre ces deux méthodes se situe au niveau de la localisation du résultat de l'opération de réduction. Avec `reduce` le résultat n'est stocké que sur un processus, tandis qu'avec `allreduce` tous les processus MPI du communicateur obtiennent le résultat à la fin (c'est un `reduce` suivant d'un `scatter`).\n",
    "\n",
    "Pour ces deux fonctions il est nécessaire de fournir un argument supplémentaire par rapport aux atres fonctions de communication, il s'agit de l'opération à effectuer. Les principales opérations disponible dans `mpi4py` sont :\n",
    "\n",
    "* MPI.MAX - retourne l'élement maximum\n",
    "* MPI.MIN - retourne l'élement minimum\n",
    "* MPI.SUM - retourne la somme des éléments\n",
    "* MPI.PROD - retourne la multiplication de tous les élements\n",
    "* MPI.MAXLOC - retourne la valeur maximale et le rang du processus sur lequel elle se trouve\n",
    "* MPI.MINLOC - retourne la valeur minimum et le rang du processus sur lequel elle se trouve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "Proc 0 data = [0. 0. 0. 0. 0.]\n",
      "Proc 0 reduce results = [6. 6. 6. 6. 6.]\n",
      "[stdout:1] Proc 1 data = [1. 1. 1. 1. 1.]\n",
      "[stdout:2] Proc 2 data = [2. 2. 2. 2. 2.]\n",
      "[stdout:3] Proc 3 data = [3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "root_rank = 0\n",
    "n = 5\n",
    "data = np.ones(n)*rank\n",
    "print(f\"Proc {rank} data = {data}\")\n",
    "recv = np.zeros(n)\n",
    "comm.Reduce(data, recv, root=root_rank, op=MPI.SUM)\n",
    "if rank == root_rank:\n",
    "    print(f\"Proc {rank} reduce results = {recv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "Proc 0 data = [0. 0. 0. 0. 0.]\n",
      "Proc 0 reduce results = [6. 6. 6. 6. 6.]\n",
      "[stdout:1] \n",
      "Proc 1 data = [1. 1. 1. 1. 1.]\n",
      "Proc 1 reduce results = [6. 6. 6. 6. 6.]\n",
      "[stdout:2] \n",
      "Proc 2 data = [2. 2. 2. 2. 2.]\n",
      "Proc 2 reduce results = [6. 6. 6. 6. 6.]\n",
      "[stdout:3] \n",
      "Proc 3 data = [3. 3. 3. 3. 3.]\n",
      "Proc 3 reduce results = [6. 6. 6. 6. 6.]\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "root_rank = 0\n",
    "n = 5\n",
    "data = np.ones(n)*rank\n",
    "print(f\"Proc {rank} data = {data}\")\n",
    "recv = np.zeros(n)\n",
    "comm.Allreduce(data, recv, op=MPI.SUM)\n",
    "\n",
    "print(f\"Proc {rank} reduce results = {recv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple de base : calcul de $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'exemple qui suit nous allons voir comment calculer $\\pi$ en parallèle à mémoire distribuée en utilisant MPI. \n",
    "Tout d'abord rappelons que le calcul de $pi$ peut se faire en calculant l'intégrale suivante \n",
    "\n",
    "$$ \\pi =  \\int_{0}^{1} \\frac{4}{1+x^2} $$\n",
    "\n",
    "En utilisant un schéma d'intégration, type rectangle nous pouvons approximer cette intégrale de la manière suivante : \n",
    "\n",
    "$$ \\pi \\simeq \\sum_{i=0}^{N-1}   \\frac{1}{N} \\cdot \\frac{4}{1+ \\frac{ i + 0.5}{N} } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une implémentation possible est la suivante : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pi_sequential( nbpoint ):\n",
    "    s = 0\n",
    "    l = 1./nbpoint\n",
    "    for i in range(nbpoint):\n",
    "        x = l * ( i + 0.5 )\n",
    "        s += l * ( 4. / (1. + x**2 ) )\n",
    "    return s\n",
    "\n",
    "print(f\"PI = {compute_pi_sequential(1000000)}\")\n",
    "\n",
    "%timeit compute_pi_sequential(100000)\n",
    "%timeit compute_pi_sequential(1000000)\n",
    "%timeit compute_pi_sequential(10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate alors que l'on a, comme on peut s'y attendre, une complexité linéaire de l'algorithme. La question en suspens est comment accélérer le calcul de $\\pi$. Dans le notebook précédent nous avons vu comment faire cela en utilisant des threads. Nous allons voir ici une autre approche qui consiste, comme vous vous en doutez, à utiliser du parallélisme à mémoire distribuée. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première étape consiste, comme présenté précédemment, a initialiser notre grappe de calcul. On rappel que cette étape est spécifique au fait que l'on soit dans un notebook. Dans un environnement python classique cette étape n'existe pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as parallel\n",
    "clients = parallel.Client(profile=\"mpi\")\n",
    "clients.block = True  # use synchronous computations\n",
    "print(clients.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "def compute_pi_mpi( nbbloc ):\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    nbprocs = comm.Get_size()\n",
    "\n",
    "    nbbloc = 1000000\n",
    "\n",
    "    start = (rank*nbbloc)//nbprocs;\n",
    "    end = ((rank+1)*nbbloc)//nbprocs;\n",
    "\n",
    "    if rank == nbprocs - 1:\n",
    "        end += nbbloc%nbprocs\n",
    "    \n",
    "    somme = 0\n",
    "    largeur = 1./nbbloc\n",
    "    for i in range(start, end):\n",
    "        x = largeur*(i+0.5);\n",
    "        somme += largeur*(4.0 / (1.0 + x*x))\n",
    "\n",
    "    \n",
    "    send = np.array([somme])\n",
    "    receive = np.zeros(1)\n",
    "\n",
    "    comm.Allreduce(send, receive, MPI.SUM)\n",
    "    return receive[0]\n",
    "\n",
    "%timeit compute_pi_mpi(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
